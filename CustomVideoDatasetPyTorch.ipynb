{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custome Video Dataset for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about usage:\n",
    "\n",
    "This code is to create a custom video dataset to train deeplearning models \n",
    "using PyTorch on consecutive video frames extracted from a video. This code expects the extracted video frames in separate folders. For example, video1's frames will be in a folder named 'video1'. You can use OpenCV to extract video frames and save inside folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import zipfile\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directories, expects the following structure\n",
    "\n",
    "# real_videos > video1 > video_frames,\n",
    "#                video 2 > video_frames,\n",
    "#                 ...\n",
    "train_dir_real = 'path_to_data/Video_Dataset/real/'\n",
    "train_dir_fake = 'path_to_data/Video_Dataset/fake/'\n",
    "\n",
    "# gets paths to directories as list\n",
    "train_list_real = glob.glob(os.path.join(train_dir_real,'*'))\n",
    "train_list_fake = glob.glob(os.path.join(train_dir_fake,'*'))\n",
    "\n",
    "# creates full training list\n",
    "train_list = []\n",
    "train_list.extend(train_list_real)\n",
    "train_list.extend(train_list_fake)\n",
    "random.shuffle(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Data Real: {len(train_list_real)}\")\n",
    "print(f\"Train Data Fake: {len(train_list_fake)}\")\n",
    "print(f\"Train Data: {len(train_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels from path\n",
    "labels = [path.split('/')[-2].split('.')[0] for path in train_list]\n",
    "\n",
    "# create train and validation sets\n",
    "train_list, valid_list = train_test_split(train_list, \n",
    "                                          test_size=0.2,\n",
    "                                          stratify=labels,\n",
    "                                          random_state=173)\n",
    "\n",
    "print(f\"Train Data: {len(train_list)}\")\n",
    "print(f\"Validation Data: {len(valid_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor()    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset class, expects 6 frames from each video\n",
    "class DeepFakeSet(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.file_list)\n",
    "        return self.filelength\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get all sorted frames from a single video\n",
    "        frames_in_video = sorted(glob.glob(self.file_list[idx] +'/*.png'))\n",
    "        \n",
    "        # empty tensor\n",
    "        concated_images_per_video = torch.zeros(1, 3, 299, 299)\n",
    "        \n",
    "        # makes sure we have 6 frames, if we have less than 6 frames in any given video, we will copy \n",
    "        # consecutive frames to reach the total tally of 6 frames, which our model expects\n",
    "        frame_check = 0\n",
    "        for i in range(len(frames_in_video)):\n",
    "            \n",
    "            if len(frames_in_video) == 4 and frame_check != 3:\n",
    "                for j in range(3):\n",
    "                    frame_check +=1\n",
    "                    img_path = frames_in_video[i]\n",
    "                    img = Image.open(img_path)\n",
    "                    img_transformed = self.transform(img)\n",
    "                    img_transformed = img_transformed.unsqueeze(0)\n",
    "                    concated_images_per_video = torch.cat((concated_images_per_video, img_transformed), dim=0)\n",
    "            \n",
    "            elif len(frames_in_video) == 5 and frame_check != 2:\n",
    "                for j in range(2):    \n",
    "                    frame_check+=1\n",
    "                    img_path = frames_in_video[i]\n",
    "                    img = Image.open(img_path)\n",
    "                    img_transformed = self.transform(img)\n",
    "                    img_transformed = img_transformed.unsqueeze(0)\n",
    "                    concated_images_per_video = torch.cat((concated_images_per_video, img_transformed), dim=0)\n",
    "            else:\n",
    "                img_path = frames_in_video[i]\n",
    "                img = Image.open(img_path)\n",
    "                img_transformed = self.transform(img)\n",
    "                img_transformed = img_transformed.unsqueeze(0)\n",
    "                concated_images_per_video = torch.cat((concated_images_per_video, img_transformed), dim=0)\n",
    "        \n",
    "        label = img_path.split(\"/\")[-3]\n",
    "        label = 1 if label == \"real\" else 0\n",
    "        \n",
    "        # take concatenated frame tensor, leave the zero tansor behind (we created zero tensor befor the for loop)\n",
    "        return concated_images_per_video[1:], label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DeepFakeSet(train_list, transform=train_transforms)\n",
    "valid_data = DeepFakeSet(valid_list, transform=val_transforms)\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
